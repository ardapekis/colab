{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language_model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ardapekis/colab/blob/master/language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9NvvGNnpQWhn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Uncomment this if you need to install dependencies.\n",
        "# GPU Version\n",
        "!pip3 install torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3tuf6cGrRfNM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run this if you need to download the training data\n",
        "\n",
        "# Download and clean Karl Marx\n",
        "!wget https://www.gutenberg.org/files/61/61.txt\n",
        "with open('61.txt', 'r') as f:\n",
        "  lines = f.readlines()\n",
        "start_index = lines.index(\"MANIFESTO OF THE COMMUNIST PARTY\\n\")\n",
        "end_index = lines.index(\"           WORKING MEN OF ALL COUNTRIES, UNITE!\\n\")\n",
        "with open('marx.txt', 'w') as f:\n",
        "  f.writelines(lines[start_index:end_index+1])\n",
        "\n",
        "# Download and clean Confucius\n",
        "!wget http://www.gutenberg.org/cache/epub/3330/pg3330.txt\n",
        "with open('pg3330.txt', 'r') as f:\n",
        "  lines = f.readlines()\n",
        "start_index = lines.index(\"CONFUCIAN ANALECTS.\\n\")\n",
        "end_index = lines.index(\"know men.'\\n\")\n",
        "with open('confucius.txt', 'w') as f:\n",
        "  f.writelines(lines[start_index:end_index+1])\n",
        "  \n",
        "# Download and clean Robert Frost\n",
        "!wget http://www.gutenberg.org/cache/epub/3021/pg3021.txt\n",
        "with open('pg3021.txt', 'r') as f:\n",
        "  lines = f.readlines()\n",
        "start_index = lines.index(\"A BOY'S WILL\\n\")\n",
        "end_index = lines.index(\"    Of a love or a season?\\n\")\n",
        "with open(\"frost.txt\", 'w') as f:\n",
        "  f.writelines(lines[start_index:end_index+1])\n",
        "  \n",
        "# Download and clean Sun Tzu\n",
        "!wget http://www.gutenberg.org/cache/epub/44024/pg44024.txt\n",
        "with open('pg44024.txt', 'r') as f:\n",
        "  lines = f.readlines()\n",
        "start_index = lines.index(\"THE ARTICLES OF SUNTZU\\n\")\n",
        "end_index = lines.index(\"Wei shook the heavens.\\n\")\n",
        "with open('suntzu.txt', 'w') as f:\n",
        "  f.writelines(lines[start_index:end_index+1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "og_gGsi1eddg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "with open('marx.txt', 'r') as f:\n",
        "  marx = f.read().encode('ascii', 'ignore')\n",
        "with open('confucius.txt', 'r') as f:\n",
        "  confucius = f.read().encode('ascii', 'ignore')\n",
        "with open('frost.txt', 'r') as f:\n",
        "  frost = f.read().encode('ascii', 'ignore')\n",
        "with open('suntzu.txt', 'r') as f:\n",
        "  suntzu = f.read().encode('ascii', 'ignore')\n",
        "\n",
        "texts = [marx, confucius, frost, suntzu]\n",
        "authors = [\"marx\", \"confucius\", \"frost\", \"suntzu\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wntii_iDSVJz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Helper code for word-level language modeling\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "      \n",
        "class Corpus(object):\n",
        "    def __init__(self, texts):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.texts = [self.tokenize(text) for text in texts]\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        # Add words to the dictionary\n",
        "        tokens = 0\n",
        "        words = text.split()\n",
        "        tokens += len(words)\n",
        "        for word in words:\n",
        "            self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        ids = torch.LongTensor(tokens)\n",
        "        token = 0\n",
        "        words = text.split()\n",
        "        for word in words:\n",
        "            ids[token] = self.dictionary.word2idx[word]\n",
        "            token += 1\n",
        "\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BhlfTTlQQ0lk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import randint\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pEhxMqlFwghn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batch(params):\n",
        "  text, author, seq_len, batch_size = params\n",
        "  batch = []\n",
        "  for i in range(batch_size):\n",
        "    start = randint(0, len(text) - seq_len)\n",
        "    batch.append((author, text[start:(start+seq_len)]))\n",
        "  return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ctqppLmfontV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharLSTM(nn.Module):\n",
        "  def __init__(self, num_authors, author_emb_dim, num_characters, character_emb_dim, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    assert num_characters == output_size\n",
        "    self.num_authors = num_authors\n",
        "    self.author_emb_dim = author_emb_dim\n",
        "    self.num_characters = num_characters\n",
        "    self.character_emb_dim = character_emb_dim\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.author_embedding = nn.Linear(num_authors, author_emb_dim)\n",
        "    self.character_embedding = nn.Linear(num_characters, character_emb_dim)\n",
        "    self.hidden_init = nn.Parameter(torch.randn([self.hidden_size]))\n",
        "    self.cell_init = nn.Parameter(torch.randn([self.hidden_size]))\n",
        "    self.forget_gate = nn.Linear(author_emb_dim + character_emb_dim + hidden_size, hidden_size)\n",
        "    self.input_gate = nn.Linear(author_emb_dim + character_emb_dim + hidden_size, hidden_size)\n",
        "    self.update_gate = nn.Linear(author_emb_dim + character_emb_dim + hidden_size, hidden_size)\n",
        "    self.output_gate = nn.Linear(author_emb_dim + character_emb_dim + hidden_size, hidden_size)\n",
        "    self.output_mlp = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, author, text, forced=True):\n",
        "    # Author is a one-hot / many-hot encoding of the text's author (batch x seq_length x num_authors)\n",
        "    # Text is a batch x seq_length x num_characters\n",
        "    author_emb = self.author_embedding(author)\n",
        "    char_emb = self.character_embedding(text)\n",
        "        \n",
        "    # This is the input sequence for the LSTM\n",
        "    input_sequence = torch.cat([author_emb, char_emb], 2)\n",
        "    \n",
        "    # The initial state of the LSTM\n",
        "    hidden_init = self.hidden_init.expand(author.shape[0], -1)\n",
        "    cell_init = self.cell_init.expand(author.shape[0], -1)\n",
        "    \n",
        "    # Keep track of the hidden output, cell memory and output over time\n",
        "    hidden_sequence = [hidden_init]\n",
        "    cell_sequence = [cell_init]\n",
        "    output_sequence = [text[:, 0] + 1e-1]\n",
        "    \n",
        "    # Do the LSTM\n",
        "    # For each sequence step...\n",
        "    for i, item in enumerate(input_sequence.permute(1, 0, 2)):\n",
        "      # Get the state information from the previous step\n",
        "      prev_hidden = hidden_sequence[-1]\n",
        "      cell = cell_sequence[-1]\n",
        "\n",
        "      if not forced:\n",
        "        item = torch.cat([author_emb[:, i], self.character_embedding(nn.functional.softmax(output_sequence[-1], 1))], 1)\n",
        "      # Concatenate the input with the previous output\n",
        "      lstm_input = torch.cat([item, prev_hidden], 1)\n",
        "      \n",
        "      # Calculate the LSTM Gates\n",
        "      forget_vector = torch.sigmoid(self.forget_gate(lstm_input))\n",
        "      input_vector = torch.sigmoid(self.input_gate(lstm_input))\n",
        "      update_vector = torch.tanh(self.update_gate(lstm_input))\n",
        "      output_vector = torch.sigmoid(self.output_gate(lstm_input))\n",
        "      \n",
        "      # Update the cell's memory based on the gates\n",
        "      cell = forget_vector                  * cell\n",
        "      cell = (input_vector * update_vector) + cell\n",
        "      \n",
        "      # We are done updating the cell's memory\n",
        "      cell_sequence.append(cell)\n",
        "      \n",
        "      # Hidden output is detemined by cell memory and output gate\n",
        "      hidden = output_vector * torch.tanh(cell)\n",
        "      \n",
        "      # We are done calculating this step of the LSTM\n",
        "      hidden_sequence.append(hidden)\n",
        "      \n",
        "      # Calculate the final output\n",
        "      # Translates LSTM output to character prediction\n",
        "      output = self.output_mlp(hidden)\n",
        "      output_sequence.append(output)\n",
        "      \n",
        "    prediction_sequence = torch.stack(output_sequence, 1)\n",
        "    return prediction_sequence\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hrqZOJc9oqZH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "pool_size = len(authors) * 1\n",
        "pool = multiprocessing.Pool(pool_size)\n",
        "\n",
        "\n",
        "def train_epoch(model, texts, authors, seq_len, batch_size, batches_per_epoch, force=True):\n",
        "  model = model.to(device)\n",
        "  optimizer = optim.RMSprop(model.parameters(), lr=2e-3, momentum=0.5)\n",
        "  running_loss = 0.0\n",
        "  \n",
        "  for i in range(batches_per_epoch):\n",
        "    # Grab the data. Use multiple workers\n",
        "    batch = [item for sublist in list(pool.map(get_batch, [(texts[i%model.num_authors], i%model.num_authors, seq_len, batch_size//pool_size) for i in range(pool_size)])) for item in sublist]\n",
        "    \n",
        "    # Format the data into one-hot vectors\n",
        "    author_sample, text_samples = zip(*batch)\n",
        "    author_one_hot = [torch.zeros([seq_len, model.num_authors]).to(device).scatter_(1, torch.LongTensor(seq_len * [[author]]).to(device), 1) for author in author_sample]\n",
        "    author_in = torch.stack(author_one_hot, 0)\n",
        "    text_one_hot = [torch.zeros([seq_len, model.num_characters]).to(device).scatter_(1, torch.LongTensor(np.frombuffer(text, dtype=np.uint8)).to(device).unsqueeze(1).clamp(max=model.num_characters-1), 1) for text in text_samples]\n",
        "    text_in = torch.stack(text_one_hot, 0)\n",
        "\n",
        "    # Predict\n",
        "    optimizer.zero_grad()\n",
        "    output = model(author_in, text_in, forced=force)\n",
        "    \n",
        "    # Calculate error\n",
        "    output = output[:, :-1, :]\n",
        "    target = torch.stack([torch.LongTensor(np.frombuffer(text, dtype=np.uint8)[1:]).to(device) for text in text_samples], 0)\n",
        "    loss = torch.tensor(0.0).to(device)\n",
        "    for i in range(target.shape[1]):\n",
        "      loss = loss + criterion(output[:, i], target[:, i])\n",
        "      \n",
        "    # Perform gradient descent\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 8)\n",
        "    optimizer.step()\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "    \n",
        "  for i in range(len(authors)):\n",
        "    print(\"{}: \".format(authors[i]) + \"\".join(list(map(chr, torch.argmax(output[i * (batch_size//4)], 1).to(torch.uint8).tolist()))))\n",
        "    \n",
        "  return running_loss / (batches_per_epoch * seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9z_4K5weivb0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model = CharLSTM(4, 32, 128, 128, 128, 128)\n",
        "\n",
        "for epoch in range(16):\n",
        "  print(\"Epoch #{}\".format(epoch))\n",
        "  start = timer()\n",
        "  loss = train_epoch(model, texts, authors, 128, 1024, 16)\n",
        "  print(\"Loss: {:.4f}\".format(loss))\n",
        "  print(\"Time: {:.1f} seconds\".format(timer()-start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cjNmsVAhxhZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(16):\n",
        "  print(\"Epoch #{}\".format(epoch))\n",
        "  start = timer()\n",
        "  loss = train_epoch(model, texts, authors, 128, 1024, 16, force=False)\n",
        "  print(\"Loss: {:.4f}\".format(loss))\n",
        "  print(\"Time: {:.1f} seconds\".format(timer()-start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PTA9eMMrf1jF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}